{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(1,6):\n",
    "    with open('/Users/minoh/Documents/Usyd2018-2/COMP5703 Capstone/data/r3/%d.json'%(i),'r') as d:  #modify path\n",
    "        tem_data = json.load(d)\n",
    "    data.extend(tem_data['result']['docs'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1501\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete duplicate json objects\n",
    "\n",
    "all_data = [ each['content'] for each in data ] \n",
    "unique = [ data[ all_data.index(id) ] for id in set(all_data) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661\n"
     ]
    }
   ],
   "source": [
    "print(len(unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract content and metatdata from json \n",
    "\n",
    "title = []\n",
    "content = []\n",
    "publishedDate = []\n",
    "summary = []\n",
    "domain = []\n",
    "adultLanguage = []\n",
    "\n",
    "\n",
    "for i in range(len(unique)):\n",
    "    if 'title' in unique[i]:\n",
    "        title.append(unique[i]['title'])\n",
    "    else:\n",
    "        title.append('')\n",
    "    if 'content' in unique[i]:\n",
    "        content.append(unique[i]['content'])\n",
    "    else: \n",
    "        content.append('')\n",
    "    if 'publishedDate' in unique[i]:\n",
    "        publishedDate.append(unique[i]['publishedDate'])\n",
    "    else:\n",
    "        publishedDate.append('')\n",
    "    if 'summary' in unique[i]:        \n",
    "        summary.append(unique[i]['summary'])\n",
    "    else:\n",
    "        summary.append('')\n",
    "        \n",
    "    for j in range(len(unique[i]['indexTerms'])):\n",
    "        domain.append(unique[i]['indexTerms'][j]['name'])\n",
    "    \n",
    "    if 'adultLanguage' in unique[i]:\n",
    "        adultLanguage.append(unique[i]['adultLanguage'])\n",
    "    else:\n",
    "        adultLanguage.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661 661 661 Parents fear risk of using home to back kids' business More than 33,000 business owners are estimated to have seed finance - or ongoing financial support - from loans that are secured with their parents' h&hellip;\n"
     ]
    }
   ],
   "source": [
    "print(len(content), len(title), len(adultLanguage), title[5], summary[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use extend so it's a big flat list of vocab\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in content:\n",
    "    allwords_stemmed = tokenize_and_stem(i) #for each item in 'content', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 422896 items in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "# create a pandas DataFrame with the stemmed vocabulary as the index and the tokenized words as the column. \n",
    "# an efficient way to look up a stem and return a full token\n",
    "\n",
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              words\n",
      "today         today\n",
      "'s               's\n",
      "royal         royal\n",
      "commiss  commission\n",
      "hear       hearings\n"
     ]
    }
   ],
   "source": [
    "print(vocab_frame.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dependencies for the model is: sklearn, numpy, keras,networkx, pyplot, wordcloud, gensim, matplotlib, collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import json\n",
    "import os\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict,Counter\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import gensim, logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we start the functions in the moedl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dir,round):\n",
    "    \n",
    "    '''load original data and keep titles and content.\n",
    "    output a dict in which keys are titles and values are content'''\n",
    "    \n",
    "    data = []\n",
    "    for i in range(1,6):\n",
    "        with open(os.path.join(dir,'r%d/%d.json'%(round,i)),'r') as d:\n",
    "            tem_data = json.load(d)\n",
    "        data.extend(tem_data['result']['docs'])\n",
    "    data_dict = {data[i]['title']:data[i]['content'] for i in range(len(data))}\n",
    "    return data_dict\n",
    "\n",
    "def convert_score(nlu_dic):\n",
    "    \n",
    "    '''the model uses Watson\\'s result as features\n",
    "      Here for each article, a dict is generated in which the keys are features and values are scores\n",
    "      And the main output is a big dict containing all article dict in which keys are titles and values are article dict'''\n",
    "    \n",
    "    feature = {'categories' : ['label','score'],'concepts' : ['text','relevance'],'entities': ['text','relevance'],'keywords':['text','relevance']}\n",
    "    convert_dic = {}\n",
    "    convert = []\n",
    "    for key in nlu_dic.keys():\n",
    "        tem = {}\n",
    "        tem = {nlu_dic[key][f][i][feature[f][0]]:nlu_dic[key][f][i][feature[f][1]] for f in feature.keys() for i in range(len(nlu_dic[key][f]))}\n",
    "        convert.append(tem)\n",
    "        convert_dic[key] = tem\n",
    "            \n",
    "    return convert_dic,convert\n",
    "\n",
    "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "    \n",
    "    '''here we build a five layers autoencoder to lower dimensions of feature matrix\n",
    "    the output is antuencoder and encoder'''\n",
    "    \n",
    "    n_stacks = len(dims) - 1\n",
    "    x = Input(shape=(dims[0],), name='input')\n",
    "    h = x\n",
    "    for i in range(n_stacks-1):\n",
    "        h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(h)\n",
    "    h = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(h)  # hidden layer, features are extracted from here\n",
    "    y = h\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        y = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(y)\n",
    "    y = Dense(dims[0], kernel_initializer=init, name='decoder_0')(y)\n",
    "    return Model(inputs=x, outputs=y, name='AE'), Model(inputs=x, outputs=h, name='encoder')\n",
    "\n",
    "def give_label(labels,data):\n",
    "    '''after clustering, articles in each cluster are represented by number. \n",
    "    here we assign titles to artiles in each cluster'''\n",
    "    \n",
    "    have_label = defaultdict(list)\n",
    "    for i in range(len(labels)):\n",
    "        have_label[labels[i]].append(list(data.keys())[i])\n",
    "    return have_label\n",
    "\n",
    "\n",
    "def get_top20(dic):\n",
    "    '''after obtaining important feature, we want to remove some features that frequently appear in all clusters\n",
    "    and split those features to two layers: 1. entities 2. themes'''\n",
    "\n",
    "    \n",
    "    features = ['concepts','categories','entities','keywords']\n",
    "    theme_dic = defaultdict(dict)\n",
    "    entities_dic = defaultdict(dict)\n",
    "    \n",
    "    themes = []\n",
    "    entities = []\n",
    "    \n",
    "    \n",
    "    tem = []\n",
    "    tem_score = {}\n",
    "    \n",
    "    for key in dic.keys():\n",
    "        for feature in features:\n",
    "            tem.extend(list(dic[key][feature]))\n",
    "            \n",
    "    for word in tem:\n",
    "        tem_score[word] = tem.count(word)/10\n",
    "       \n",
    "    for key in dic.keys():\n",
    "        \n",
    "        for theme in ['concepts','categories']:\n",
    "            for word in dic[key][theme]:\n",
    "                theme_dic[key][word] = tem_score[word]\n",
    "                \n",
    "        for entity in ['entities','keywords']:\n",
    "            for word in dic[key][entity]:\n",
    "                entities_dic[key][word] = tem_score[word]\n",
    "\n",
    "    for i in range(10):\n",
    "        themes.append(sorted(theme_dic['cluster%d'%i].items(), key = lambda item:item[1])[:10])\n",
    "        \n",
    "        entities.append(sorted(entities_dic['cluster%d'%i].items(), key = lambda item:item[1])[:25])\n",
    "    return themes,entities\n",
    "\n",
    "def get_top5_sentence(kw,n,data):\n",
    "    \n",
    "    ''' after training a Word2Vec model, \n",
    "    here we compute similarities between each sentence and keywords,\n",
    "    output the top 5 similar sentences.\n",
    "    '''\n",
    "    kw_list = []\n",
    "    sentence = []\n",
    "    sentence_score = {}\n",
    "    model = word2vect(data)\n",
    "    for word in kw[n]:\n",
    "        kw_list.extend(word[0].lower().split())\n",
    "    for key in data.keys():\n",
    "        sentence.extend(data[key].split('\\n'))\n",
    "    for s in sentence:\n",
    "        tem = s.lower().split()\n",
    "        if tem != []:\n",
    "            sentence_score[s] = model.n_similarity(kw_list,tem)\n",
    "    top_5_score = sorted(sentence_score.items(), key = lambda item:item[1],reverse=True)[:5]\n",
    "    top_5 = [m[0] for m in top_5_score]\n",
    "    return top_5\n",
    "\n",
    "def get_important_feature(title_list,nlu_data):\n",
    "    \n",
    "        '''here we obtain most frequent features of each cluster\n",
    "     we have 4 types of features, and keep 20 for each which sum to 80'''\n",
    "    \n",
    "    features = {'concepts' : 'text','keywords':'text'}\n",
    "    features_lis = ['categories' ,'concepts' ,'entities','keywords']\n",
    "    \n",
    "    entities_filter = ['Location','Person']\n",
    "    keywords_filter = ['Mr','Ms']\n",
    "    \n",
    "    C = defaultdict(list)\n",
    "    result = defaultdict(list)\n",
    "    for title in title_list:\n",
    "        for i in range (len(nlu_data[title]['concepts'])):        \n",
    "            C['concepts'].append(nlu_data[title]['concepts'][i]['text'])\n",
    "                \n",
    "        for i in range (len(nlu_data[title]['keywords'])): \n",
    "            for fil in keywords_filter:\n",
    "                if fil not in nlu_data[title]['keywords'][i]['text']:\n",
    "                    C['keywords'].append(nlu_data[title]['keywords'][i]['text'])\n",
    "                \n",
    "        for i in range (len(nlu_data[title]['categories'])):        \n",
    "            C['categories'].extend(nlu_data[title]['categories'][i]['label'].split('/')[-2:])\n",
    "                \n",
    "        for i in range (len(nlu_data[title]['entities'])):\n",
    "            if nlu_data[title]['entities'][i]['type'] not in entities_filter:\n",
    "                C['entities'].append(nlu_data[title]['entities'][i]['text'])\n",
    "                \n",
    "    for feature in features_lis:\n",
    "        tem = Counter(C[feature]).most_common(20)\n",
    "            \n",
    "        tem_list = []\n",
    "    \n",
    "        for i in range(20):\n",
    "            tem_list.append(tem[i][0])\n",
    "        result[feature] = tem_list\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def init_visualisation(themes,entities,convert_dic,have_label):\n",
    "    '''\n",
    "    here we initialise some requirements of visualisetion: edges and nodes\n",
    "    edges contain some tuples in the shape (theme,entity) or (entity, article)'''\n",
    "    \n",
    "    havelabel = defaultdict(dict)\n",
    "    \n",
    "    for key in have_label.keys():\n",
    "        for i in range(len(have_label[key])):\n",
    "            havelabel[key][have_label[key][i]] = i\n",
    "            \n",
    "    feature_bag = defaultdict(list)\n",
    "    for title in convert_dic.keys():\n",
    "        for feature in convert_dic[title]:\n",
    "            feature_bag[title].extend(feature.split('/'))\n",
    "            \n",
    "    features = {'categories' : ['label','score'],'concepts' : ['text','relevance'],'entities': ['text','relevance'],'keywords':['text','relevance']}\n",
    "   \n",
    "\n",
    "    nodes = defaultdict(list)\n",
    "    edges = defaultdict(dict)\n",
    "\n",
    "\n",
    "    for i in range(len(themes)):\n",
    "        tem = {(theme[0],entity[0]):0 for theme in themes[i] for entity in entities[i]}\n",
    "        edges['cluster%d'%i] = tem\n",
    "            \n",
    "    for i in range(len(themes)):\n",
    "        for title in havelabel[i]:\n",
    "            for relation in edges['cluster%d'%i]:\n",
    "                if relation[0] in feature_bag[title] and relation[1] in feature_bag[title]:\n",
    "                    edges['cluster%d'%i][relation] +=1 \n",
    "                    \n",
    "        for title in havelabel[i].keys():\n",
    "            for entity in entities[i]:\n",
    "                if entity[0] in feature_bag[title]:\n",
    "                    if entity[0] in convert_dic[title].keys():\n",
    "                        edges['cluster%d'%i][(entity[0],havelabel[i][title])] = convert_dic[title][entity[0]]\n",
    "                    else:\n",
    "                        for key in convert_dic[title].keys():\n",
    "                            if entity[0] in key:\n",
    "                                edges['cluster%d'%i][(entity[0],havelabel[i][title])] = convert_dic[title][key]\n",
    "                            break\n",
    "    for i in range(len(themes)):\n",
    "        edges['cluster%d'%i] = {k:v for k,v in edges['cluster%d'%i].items() if v!=0}\n",
    "        for k,v in edges['cluster%d'%i].items():\n",
    "            t = max(edges['cluster%d'%i].values())\n",
    "            if v>=1:\n",
    "                    edges['cluster%d'%i][k] = v/t\n",
    "    return edges\n",
    "\n",
    "def draw_cluster(edges,n,path):\n",
    "    \n",
    "    '''here we draw the relation graph of a cluster'''\n",
    "    \n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(edges['cluster%d'%n],attr_dict = edges['cluster%d'%n])\n",
    "    plt.figure(figsize = (100,15))\n",
    "    p = nx.drawing.nx_pydot.graphviz_layout(G, prog='dot')\n",
    "    nx.draw_networkx_nodes(G,p,node_size=3000,node_color = '#F8F8FF')\n",
    "    for edge in G.edges(data = True):\n",
    "        nx.draw_networkx_edges(G,p,edgelist =[(edge[0],edge[1])],alpha = edge[2]['attr_dict'][(edge[0],edge[1])],width=3,edge_color='#4682B4')\n",
    "    nx.draw_networkx_labels(G,p,font_size=14)\n",
    "    plt.axis('off')\n",
    "    newpath = path+\"/cluster%d/\"%n\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    plt.savefig(newpath+\"cluster%d.png\"%n)\n",
    "    \n",
    "def draw_kw(edges,k,kw,path):\n",
    "    \n",
    "    '''here we draw relation graph of a theme'''\n",
    "    \n",
    "    k_e = [relation[1] for relation in edges['cluster%d'%k].keys() if kw in relation]\n",
    "    k_e.append(kw)\n",
    "    kw_dic = {k:v for k,v in edges['cluster%d'%k].items() if k[0] in k_e}\n",
    "    g = nx.DiGraph()\n",
    "    plt.figure(figsize = (100,10))\n",
    "    g.add_edges_from(kw_dic,attr_dict = kw_dic)\n",
    "    pp = nx.drawing.nx_pydot.graphviz_layout(g, prog='dot')\n",
    "    nx.draw_networkx_nodes(g,pp,node_size=3000,node_color = '#F8F8FF')\n",
    "    for edge in g.edges(data = True):\n",
    "        nx.draw_networkx_edges(g,pp,edgelist =[(edge[0],edge[1])],alpha = edge[2]['attr_dict'][(edge[0],edge[1])],width=3,edge_color='#4682B4')\n",
    "    nx.draw_networkx_labels(g,pp,font_size=14)\n",
    "    plt.axis('off')\n",
    "    newpath = path+\"/cluster%d/keywords/\"%k\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)                \n",
    "    plt.savefig(newpath+\"%s.png\"%kw)\n",
    "    \n",
    "\n",
    "def word2vect(data,kw_list):\n",
    "    '''train and return word2vec model'''\n",
    "    exception = ['surety','n']\n",
    "    split = []\n",
    "    for article in list(data.values()):\n",
    "        split.append(article.lower().split())\n",
    "    split.append(exception)\n",
    "    split.append(kw_list)\n",
    "    model = gensim.models.Word2Vec(split, min_count=1)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def get_result(nlu_data,data,path):\n",
    "    \n",
    "    '''get the results'''\n",
    "    \n",
    "    di,li = convert_score(nlu_data)  #convert features to scores\n",
    "    DV = DictVectorizer(sparse=False)\n",
    "    x = DV.fit_transform(li)           #get feature matrix\n",
    "    \n",
    "    dims=[x.shape[-1], 500, 500, 2000, 10]    #define dimension of autoencoder\n",
    "    AE,encoder = autoencoder(dims, act='relu', init='glorot_uniform')   \n",
    "    AE.compile(optimizer='adam', loss='mse')\n",
    "    AE.fit(x, x, batch_size=30, epochs=5,verbose =0)      #train autoencoder\n",
    "    x_encoder = encoder.predict(x)            #lower dimension of matrix\n",
    "    \n",
    "    n_clusters = 10       #set number of clusters \n",
    "    kmeans = KMeans(n_clusters=n_clusters,max_iter=600).fit(x_encoder)        #clustering\n",
    "    have_label = give_label(kmeans.labels_,nlu_data)        #assign titles to artitles\n",
    "    \n",
    "    important_feature = {}\n",
    "    result = defaultdict(dict)   #get important features\n",
    "    important_feature = {'cluster%d'%i:get_important_feature(have_label[i],nlu_data) for i in range(10)}\n",
    "    \n",
    "    themes,entities= get_top20(important_feature)              #get themes and entites\n",
    "    edges = init_visualisation(themes,entities,di,have_label)       #initialise requirements of visualisation\n",
    "    \n",
    "    \n",
    "    for i in range(10):        #for each cluster, output visualisations\n",
    "        \n",
    "        draw_cluster(edges,i,path)\n",
    "        c = [m[0] for m in themes[i]]\n",
    "        result['Cluster %d'%i]['theme'] = c\n",
    "        k = [m[0] for m in entities[i]]\n",
    "        result['Cluster %d'%i]['entities'] = k\n",
    "        for d in c:\n",
    "            draw_kw(edges,i,d,path)\n",
    "        keys = []\n",
    "        for t in themes:\n",
    "            tem = [k[0] for k in t]\n",
    "            keys.extend(tem)\n",
    "        for t in entities:\n",
    "            tem = [k[0] for k in t]\n",
    "            keys.extend(tem)\n",
    "        g = get_top5_sentence(keys,i,data)\n",
    "        result['Cluster %d'%i]['Sentences'] = g\n",
    "        \n",
    "    with open(path+'result.json','w') as f:      #save results of each cluster\n",
    "        json.dump(result, f)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir='/Users/tommy/Desktop/COMP5703/data/'         #modify original data path here\n",
    "\n",
    "data_r3 = load_data(dir,3)\n",
    "data_r4 = load_data(dir,4)\n",
    "data_r5 = load_data(dir,5)\n",
    "\n",
    "with open('data_r3_nlu.json','r') as d:         #load watson_nlu data\n",
    "    data_r3_nlu = json.load(d)\n",
    "with open('data_r4_nlu.json','r') as d:\n",
    "    data_r4_nlu = json.load(d)\n",
    "with open('data_r5_nlu.json','r') as d:\n",
    "    data_r5_nlu = json.load(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    " \n",
    "path = dir+'round3/'         #modify folder name 'round3' 'round4' 'round5'\n",
    "\n",
    "result = get_result(data_r3_nlu,data_r3,path)    #modify variables 'data_r?_nlu' and 'data_r?'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras import callbacks\n",
    "from keras.initializers import VarianceScaling\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_retures_keras():\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.datasets import reuters\n",
    "    max_words = 1000\n",
    "\n",
    "    print('Loading data...')\n",
    "    (x, y), (_, _) = reuters.load_data(num_words=max_words, test_split=0.)\n",
    "    print(len(x), 'train sequences')\n",
    "\n",
    "    num_classes = np.max(y) + 1\n",
    "    print(num_classes, 'classes')\n",
    "\n",
    "    print('Vectorizing sequence data...')\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    x = tokenizer.sequences_to_matrix(x, mode='binary')\n",
    "    print('x_train shape:', x.shape)\n",
    "\n",
    "    return x.astype(float), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "11228 train sequences\n",
      "46 classes\n",
      "Vectorizing sequence data...\n",
      "x_train shape: (11228, 1000)\n"
     ]
    }
   ],
   "source": [
    "x,y = load_retures_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "\n",
    "    n_stacks = len(dims) - 1\n",
    "    # input\n",
    "    x = Input(shape=(dims[0],), name='input')\n",
    "    h = x\n",
    "\n",
    "    # internal layers in encoder\n",
    "    for i in range(n_stacks-1):\n",
    "        h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(h)\n",
    "\n",
    "    # hidden layer\n",
    "    h = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(h)  # hidden layer, features are extracted from here\n",
    "\n",
    "    y = h\n",
    "    # internal layers in decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        y = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(y)\n",
    "\n",
    "    # output\n",
    "    y = Dense(dims[0], kernel_initializer=init, name='decoder_0')(y)\n",
    "\n",
    "    return Model(inputs=x, outputs=y, name='AE'), Model(inputs=x, outputs=h, name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims=[x.shape[-1], 500, 500, 2000, 10]\n",
    "AE,encoder = autoencoder(dims, act='relu', init='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/tommy/Desktop/result'\n",
    "csv_logger = callbacks.CSVLogger(save_dir + '/pretrain_log.csv')\n",
    "cb = [csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11228/11228 [==============================] - 5s 428us/step - loss: 0.0459\n",
      "Epoch 2/20\n",
      "11228/11228 [==============================] - 4s 367us/step - loss: 0.0400\n",
      "Epoch 3/20\n",
      "11228/11228 [==============================] - 4s 382us/step - loss: 0.0381\n",
      "Epoch 4/20\n",
      "11228/11228 [==============================] - 4s 375us/step - loss: 0.0371\n",
      "Epoch 5/20\n",
      "11228/11228 [==============================] - 4s 380us/step - loss: 0.0361\n",
      "Epoch 6/20\n",
      "11228/11228 [==============================] - 5s 407us/step - loss: 0.0353\n",
      "Epoch 7/20\n",
      "11228/11228 [==============================] - 5s 444us/step - loss: 0.0347\n",
      "Epoch 8/20\n",
      "11228/11228 [==============================] - 5s 425us/step - loss: 0.0342\n",
      "Epoch 9/20\n",
      "11228/11228 [==============================] - 5s 401us/step - loss: 0.0338\n",
      "Epoch 10/20\n",
      "11228/11228 [==============================] - 5s 428us/step - loss: 0.0333\n",
      "Epoch 11/20\n",
      "11228/11228 [==============================] - 5s 427us/step - loss: 0.0330\n",
      "Epoch 12/20\n",
      "11228/11228 [==============================] - 4s 399us/step - loss: 0.0327\n",
      "Epoch 13/20\n",
      "11228/11228 [==============================] - 5s 402us/step - loss: 0.0324\n",
      "Epoch 14/20\n",
      "11228/11228 [==============================] - 4s 358us/step - loss: 0.0321\n",
      "Epoch 15/20\n",
      "11228/11228 [==============================] - 4s 363us/step - loss: 0.0318\n",
      "Epoch 16/20\n",
      "11228/11228 [==============================] - 4s 375us/step - loss: 0.0316\n",
      "Epoch 17/20\n",
      "11228/11228 [==============================] - 4s 388us/step - loss: 0.0314\n",
      "Epoch 18/20\n",
      "11228/11228 [==============================] - 4s 381us/step - loss: 0.0312\n",
      "Epoch 19/20\n",
      "11228/11228 [==============================] - 5s 407us/step - loss: 0.0310\n",
      "Epoch 20/20\n",
      "11228/11228 [==============================] - 5s 435us/step - loss: 0.0308\n"
     ]
    }
   ],
   "source": [
    "AE.compile(optimizer='adam', loss='mse')\n",
    "AE.fit(x, x, batch_size=256, epochs=20, callbacks=cb)\n",
    "AE.save_weights(save_dir + '/ae_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_encoder = encoder.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def assign_cluster_label(X, Y):\n",
    "    kmeans = KMeans(n_clusters=len(set(Y))).fit(X)\n",
    "    #gmm = GaussianMixture(n_components = len(set(Y))).fit(X)\n",
    "    #gmm_labels = gmm.predict(X)\n",
    "    Y_k = np.zeros(Y.shape)\n",
    "   # Y_g = np.zeros(Y.shape)\n",
    "    for i in set(kmeans.labels_):\n",
    "        ind = kmeans.labels_ == i\n",
    "        Y_k[ind] = Counter(Y[ind]).most_common(1)[0][0] # assign label.\n",
    "    #for i in set(gmm_labels):\n",
    "    #    ind = gmm_labels == i\n",
    "    #    Y_g[ind] = Counter(Y[ind]).most_common(1)[0][0] # assign label.\n",
    "\n",
    "    print('==> Evaluate Acc and NMI ...')\n",
    "    acc_k = accuracy_score(y, Y_k)\n",
    "    nmi_k = normalized_mutual_info_score(y, Y_k)\n",
    "    print('Kmeans: Acc(NMI) = {:.4f} ({:.4f})'.format(acc_k, nmi_k))\n",
    "   # acc_g = accuracy_score(y, Y_g)\n",
    "   # nmi_g = normalized_mutual_info_score(y, Y_g)\n",
    "    #print('GMM: Acc(NMI) = {:.4f} ({:.4f})'.format(acc_g, nmi_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Evaluate Acc and NMI ...\n",
      "Kmeans: Acc(NMI) = 0.6371 (0.4283)\n"
     ]
    }
   ],
   "source": [
    "assign_cluster_label(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Evaluate Acc and NMI ...\n",
      "Kmeans: Acc(NMI) = 0.6690 (0.4752)\n",
      "GMM: Acc(NMI) = 0.6551 (0.4511)\n"
     ]
    }
   ],
   "source": [
    "assign_cluster_label(x_encoder,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
